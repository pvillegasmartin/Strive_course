{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37298ca0",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "In the same way that for many programs hellow word is the first program in spark is computing pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9287cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.14154708\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 100000000\n",
    "def inside(p):\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "pi = 4 * count / NUM_SAMPLES\n",
    "print(\"Pi is roughly\", pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eeaf2b",
   "metadata": {},
   "source": [
    "## SQL and DataFrames\n",
    "\n",
    "There are two approaches to Spark, the DataFrame approach and the RDD approach. We are going to learn the SQL approach since it is works in the way spark intends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbc38da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\pablo\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: findspark in c:\\users\\pablo\\anaconda3\\lib\\site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9550e5",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "Spark needs to use a session in order to process data in a parallel way.\n",
    "\n",
    "A session can be built in many different ways, what is going to be a difference maker for most local machines is that we need to specify to spark to either get it or create it.\n",
    "\n",
    "We will use this session to define our Spark DataFrames.\n",
    "\n",
    "When Creating DataFrames we can let spark infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8b7740",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1472/1702755658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'emotion'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iris.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[0;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misEncryptionEnabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPythonAuthSocketTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_BUFFER_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSparkBufferSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1545\u001b[0m                     answer, self._gateway_client, self._fqn, name)\n\u001b[0;32m   1546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m             raise Py4JError(\n\u001b[0m\u001b[0;32m   1548\u001b[0m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('emotion').getOrCreate()\n",
    "df = spark.read.csv('iris.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f83617",
   "metadata": {},
   "source": [
    "Or we can create our own schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47db31f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = StructType([\n",
    "                     StructField('sepal_length', DoubleType(), False),\n",
    "                     StructField('sepal_width', DoubleType(), False),\n",
    "                     StructField('petal_length', DoubleType(), False),\n",
    "                     StructField('petal_width', DoubleType(), False),\n",
    "                     StructField('type', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35b95354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('iris.csv', header = True, schema = table_schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d366c",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Spark doesn`t need the X and Y separated in the standard format.\n",
    "\n",
    "As you will see throught the notebook spark will directly operate on the DataFrame after specifying an input and aoutput column.\n",
    "\n",
    "By default Spark takes a column called deatures as the input in all classifiers and the Y column is called labels\n",
    "\n",
    "We can create the  feature column by using a vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "417c4a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|[5.0,3.6,1.4,0.2]|\n",
      "|         5.4|        3.9|         1.7|        0.4|Setosa|[5.4,3.9,1.7,0.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3|Setosa|[4.6,3.4,1.4,0.3]|\n",
      "|         5.0|        3.4|         1.5|        0.2|Setosa|[5.0,3.4,1.5,0.2]|\n",
      "|         4.4|        2.9|         1.4|        0.2|Setosa|[4.4,2.9,1.4,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|Setosa|[4.9,3.1,1.5,0.1]|\n",
      "|         5.4|        3.7|         1.5|        0.2|Setosa|[5.4,3.7,1.5,0.2]|\n",
      "|         4.8|        3.4|         1.6|        0.2|Setosa|[4.8,3.4,1.6,0.2]|\n",
      "|         4.8|        3.0|         1.4|        0.1|Setosa|[4.8,3.0,1.4,0.1]|\n",
      "|         4.3|        3.0|         1.1|        0.1|Setosa|[4.3,3.0,1.1,0.1]|\n",
      "|         5.8|        4.0|         1.2|        0.2|Setosa|[5.8,4.0,1.2,0.2]|\n",
      "|         5.7|        4.4|         1.5|        0.4|Setosa|[5.7,4.4,1.5,0.4]|\n",
      "|         5.4|        3.9|         1.3|        0.4|Setosa|[5.4,3.9,1.3,0.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3|Setosa|[5.1,3.5,1.4,0.3]|\n",
      "|         5.7|        3.8|         1.7|        0.3|Setosa|[5.7,3.8,1.7,0.3]|\n",
      "|         5.1|        3.8|         1.5|        0.3|Setosa|[5.1,3.8,1.5,0.3]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numericCols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da68e68",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "We can use a string indexes in the same way as the vector assembler to ordinally encode our types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1043de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|         features|labelIndex|\n",
      "+------------+-----------+------------+-----------+------+-----------------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|[5.1,3.5,1.4,0.2]|       0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|[4.9,3.0,1.4,0.2]|       0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|[4.7,3.2,1.3,0.2]|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|       0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|[5.0,3.6,1.4,0.2]|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|Setosa|[5.4,3.9,1.7,0.4]|       0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|Setosa|[4.6,3.4,1.4,0.3]|       0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2|Setosa|[5.0,3.4,1.5,0.2]|       0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|Setosa|[4.4,2.9,1.4,0.2]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|Setosa|[4.9,3.1,1.5,0.1]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'type', outputCol = 'labelIndex')\n",
    "df = label_stringIdx.fit(df).transform(df)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682ec23",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "Spark Dataframes come pre-equipped with a random split function that will give you as many portions as specified.\n",
    "\n",
    "The proportions for each portion are passed in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19be27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9188c",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "Many Spark classifiers unfortunatelly do not handle good multylabel classification so be very carefull with which you choose.\n",
    "\n",
    "they can all be found here: https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2708db9",
   "metadata": {},
   "source": [
    "## Specifying input and target\n",
    "\n",
    "As I said the default names are features an label, but we can also specify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee151389",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3deb06a",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "It is done in the same way as SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0783059",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5915b4",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "    \n",
    "This part is a little different. Spark will not output a prediction vector, it will direclty add a column to the DataFrame.\n",
    "\n",
    "To predict we call the method 'transform' from the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa20f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ae586",
   "metadata": {},
   "source": [
    "## Selecting\n",
    "\n",
    "This structures are built to be parallelized in the CPU so we cannot access them in a standard fashion.\n",
    "\n",
    "To get a subset of columns we need to use select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "deee0a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+--------------------+----------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|labelIndex|       rawPrediction|prediction|         probability|\n",
      "+------------+-----------+------------+-----------+----------+--------------------+----------+--------------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         4.5|        2.3|         1.3|        0.3|       0.0|      [19.0,0.0,1.0]|       0.0|     [0.95,0.0,0.05]|\n",
      "|         4.6|        3.6|         1.0|        0.2|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         4.9|        3.1|         1.5|        0.2|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         4.9|        3.6|         1.4|        0.1|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.0|        3.4|         1.6|        0.4|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.0|        3.5|         1.3|        0.3|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.0|        3.5|         1.6|        0.6|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.1|        3.3|         1.7|        0.5|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.1|        3.5|         1.4|        0.3|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.1|        3.7|         1.5|        0.4|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.1|        3.8|         1.6|        0.2|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.2|        3.4|         1.4|        0.2|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.4|        3.7|         1.5|        0.2|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.4|        3.9|         1.3|        0.4|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.5|        2.4|         3.7|        1.0|       1.0|      [0.0,20.0,0.0]|       1.0|       [0.0,1.0,0.0]|\n",
      "|         5.6|        2.7|         4.2|        1.3|       1.0|      [0.0,20.0,0.0]|       1.0|       [0.0,1.0,0.0]|\n",
      "|         5.6|        2.8|         4.9|        2.0|       2.0|[0.0,2.0714285714...|       2.0|[0.0,0.1035714285...|\n",
      "|         5.6|        2.9|         3.6|        1.3|       1.0|      [0.0,20.0,0.0]|       1.0|       [0.0,1.0,0.0]|\n",
      "|         5.7|        2.5|         5.0|        2.0|       2.0|[0.0,2.0714285714...|       2.0|[0.0,0.1035714285...|\n",
      "|         5.7|        3.8|         1.7|        0.3|       0.0|      [20.0,0.0,0.0]|       0.0|       [1.0,0.0,0.0]|\n",
      "|         5.7|        4.4|         1.5|        0.4|       0.0|      [19.0,1.0,0.0]|       0.0|     [0.95,0.05,0.0]|\n",
      "|         5.8|        2.7|         4.1|        1.0|       1.0|[0.0,19.947368421...|       1.0|[0.0,0.9973684210...|\n",
      "|         5.9|        3.2|         4.8|        1.8|       1.0|[0.0,1.0714285714...|       2.0|[0.0,0.0535714285...|\n",
      "|         6.0|        3.0|         4.8|        1.8|       2.0|[0.0,1.0714285714...|       2.0|[0.0,0.0535714285...|\n",
      "+------------+-----------+------------+-----------+----------+--------------------+----------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'labelIndex', 'rawPrediction', 'prediction', 'probability').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "216843ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|labelIndex|prediction|\n",
      "+----------+----------+\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"labelIndex\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ebdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating\n",
    "\n",
    "In a very familiar format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "03efeb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9754807071880243\n",
      "Test Error = 0.024519292811975735\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
