{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nChatbots are essential for speeding up user assistance and reducing waiting times. Chatbots can quickly extract important information such as demographics, symptoms, health insurance information and assist any patients by making appointments with specialists.\\n\\nImagine having to design a tool that allows preliminary assistance for those who must access a treatment path or must make a reservation for a specialist visit.\\n\\nCreate a dataset using the template provided as a base and prepare at least 5 different intents with 4/5 responses each.\\n\\nThe final result must ensure that users can have a dialogue of at least 3 questions and 3 answers consistent with the context.\\n\\nExample\\nA: Hello MedAssistant.\\nB: Hello. How can I help you?\\nA: I don't feel well.\\nB: Do you have any symptoms?\\nA: I have cough and nausea.\\nB: Do you want to book an appointment?\\nA: Yes, please, for tomorrow.\\n\\n\\nInfo:\\n- Feel free to change or arrange a new dataset of intents\\n- Try experimenting and tuning with the hyperparameters\\n- Feel free to use or change the code you've seen during the morning session\\n- TBD = To be done (from you!) :)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Lecture @ Strive School - 21st July 2021\n",
    "# CHATBOT with Pytorch\n",
    "\n",
    "'''\n",
    "Chatbots are essential for speeding up user assistance and reducing waiting times. Chatbots can quickly extract important information such as demographics, symptoms, health insurance information and assist any patients by making appointments with specialists.\n",
    "\n",
    "Imagine having to design a tool that allows preliminary assistance for those who must access a treatment path or must make a reservation for a specialist visit.\n",
    "\n",
    "Create a dataset using the template provided as a base and prepare at least 5 different intents with 4/5 responses each.\n",
    "\n",
    "The final result must ensure that users can have a dialogue of at least 3 questions and 3 answers consistent with the context.\n",
    "\n",
    "Example\n",
    "A: Hello MedAssistant.\n",
    "B: Hello. How can I help you?\n",
    "A: I don't feel well.\n",
    "B: Do you have any symptoms?\n",
    "A: I have cough and nausea.\n",
    "B: Do you want to book an appointment?\n",
    "A: Yes, please, for tomorrow.\n",
    "\n",
    "\n",
    "Info:\n",
    "- Feel free to change or arrange a new dataset of intents\n",
    "- Try experimenting and tuning with the hyperparameters\n",
    "- Feel free to use or change the code you've seen during the morning session\n",
    "- TBD = To be done (from you!) :)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "from nltk_utils import bag_of_words, tokenize, stem\n",
    "from model import NeuralNet\n",
    "\n",
    "# STEP 0: find intents patterns\n",
    "\n",
    "with open('medical_intents.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Pre-process of the input\n",
    "\n",
    "# lower case? stemming? stopwords?\n",
    "# TBD\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    #remove punctuation from text and remove stop words from text\n",
    "    text = ' '.join([token.lemma_ for token in nlp(text) if not token.is_punct])\n",
    "    return text\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "patterns = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "# TBD: loop through each sentence in our intents patterns, create a list of tags and define the patterns\n",
    "    tags.append(intent['tag'])\n",
    "    for pattern in intent['patterns']:\n",
    "        pattern = preprocess(pattern)\n",
    "        words = nlp(pattern)\n",
    "        patterns.append(([token.text for token in words],intent['tag']))\n",
    "        all_words.extend(words)\n",
    "all_words = sorted(set([word.text for word in all_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Define training data through a bag of words\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in patterns:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'  ': 0.0,\n",
       " '24': 0.0,\n",
       " '3': 0.0,\n",
       " '37.8c': 0.0,\n",
       " 'I': 0.0,\n",
       " 'a': 0.0,\n",
       " 'abdomen': 0.0,\n",
       " 'abdominal': 0.0,\n",
       " 'activity': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'an': 0.0,\n",
       " 'and': 0.0,\n",
       " 'ankle': 0.0,\n",
       " 'anxiety': 0.0,\n",
       " 'anyone': 0.0,\n",
       " 'appetite': 0.0,\n",
       " 'as': 0.0,\n",
       " 'ascite': 0.0,\n",
       " 'attack': 0.0,\n",
       " 'bad': 0.0,\n",
       " 'be': 0.0,\n",
       " 'black': 0.0,\n",
       " 'bleed': 0.0,\n",
       " 'bleeding': 0.0,\n",
       " 'block': 0.0,\n",
       " 'blocked': 0.0,\n",
       " 'blood': 0.0,\n",
       " 'blotchy': 0.0,\n",
       " 'brain': 0.0,\n",
       " 'breath': 0.0,\n",
       " 'bruise': 0.0,\n",
       " 'build': 0.0,\n",
       " 'call': 0.0,\n",
       " 'can': 0.0,\n",
       " 'change': 0.0,\n",
       " 'chest': 0.0,\n",
       " 'club': 0.0,\n",
       " 'confusion': 0.0,\n",
       " 'continuous': 0.0,\n",
       " 'cough': 0.0,\n",
       " 'crack': 0.0,\n",
       " 'curve': 0.0,\n",
       " 'day': 0.0,\n",
       " 'delusion': 0.0,\n",
       " 'depression': 0.0,\n",
       " 'diarrhoea': 0.0,\n",
       " 'disinter': 0.0,\n",
       " 'dizziness': 0.0,\n",
       " 'drug': 0.0,\n",
       " 'dry': 0.0,\n",
       " 'due': 0.0,\n",
       " 'easily': 0.0,\n",
       " 'episode': 0.0,\n",
       " 'eye': 0.0,\n",
       " 'face': 0.0,\n",
       " 'fatigue': 0.0,\n",
       " 'feel': 0.0,\n",
       " 'fever': 0.0,\n",
       " 'finger': 0.0,\n",
       " 'fingertip': 0.0,\n",
       " 'flashback': 0.0,\n",
       " 'fluid': 0.0,\n",
       " 'foot': 0.0,\n",
       " 'for': 0.0,\n",
       " 'frequent': 0.0,\n",
       " 'generally': 0.0,\n",
       " 'good': 0.0,\n",
       " 'great': 0.0,\n",
       " 'gum': 0.0,\n",
       " 'hair': 0.0,\n",
       " 'hallucination': 0.0,\n",
       " 'have': 0.0,\n",
       " 'hello': 0.0,\n",
       " 'help': 0.0,\n",
       " 'hi': 1.0,\n",
       " 'high': 0.0,\n",
       " 'hive': 0.0,\n",
       " 'hoarse': 0.0,\n",
       " 'hour': 0.0,\n",
       " 'how': 0.0,\n",
       " 'in': 0.0,\n",
       " 'inability': 0.0,\n",
       " 'increase': 0.0,\n",
       " 'insomnia': 0.0,\n",
       " 'internal': 0.0,\n",
       " 'itchy': 0.0,\n",
       " 'jaundice': 0.0,\n",
       " 'know': 0.0,\n",
       " 'last': 0.0,\n",
       " 'leg': 0.0,\n",
       " 'lip': 0.0,\n",
       " 'loss': 0.0,\n",
       " 'memory': 0.0,\n",
       " 'more': 0.0,\n",
       " 'muscle': 0.0,\n",
       " 'my': 0.0,\n",
       " 'nail': 0.0,\n",
       " 'name': 0.0,\n",
       " 'nausea': 0.0,\n",
       " 'need': 0.0,\n",
       " 'new': 0.0,\n",
       " 'nightmare': 0.0,\n",
       " 'nose': 0.0,\n",
       " 'nosebleed': 0.0,\n",
       " 'not': 0.0,\n",
       " 'of': 0.0,\n",
       " 'or': 0.0,\n",
       " 'pain': 0.0,\n",
       " 'palm': 0.0,\n",
       " 'personality': 0.0,\n",
       " 'problem': 0.0,\n",
       " 'raise': 0.0,\n",
       " 'rash': 0.0,\n",
       " 'red': 0.0,\n",
       " 'regular': 0.0,\n",
       " 'runny': 0.0,\n",
       " 's': 0.0,\n",
       " 'sensitivity': 0.0,\n",
       " 'shape': 0.0,\n",
       " 'shiver': 0.0,\n",
       " 'shortness': 0.0,\n",
       " 'should': 0.0,\n",
       " 'sick': 0.0,\n",
       " 'significant': 0.0,\n",
       " 'skin': 0.0,\n",
       " 'sleep': 0.0,\n",
       " 'sneeze': 0.0,\n",
       " 'some': 0.0,\n",
       " 'sore': 0.0,\n",
       " 'stool': 0.0,\n",
       " 'strange': 0.0,\n",
       " 'such': 0.0,\n",
       " 'suicidal': 0.0,\n",
       " 'swell': 0.0,\n",
       " 'swollen': 0.0,\n",
       " 'tarry': 0.0,\n",
       " 'tell': 0.0,\n",
       " 'temperature': 0.0,\n",
       " 'tendency': 0.0,\n",
       " 'tense': 0.0,\n",
       " 'than': 0.0,\n",
       " 'thatâ€': 0.0,\n",
       " 'the': 0.0,\n",
       " 'there': 0.0,\n",
       " 'thought': 0.0,\n",
       " 'throat': 0.0,\n",
       " 'tightness': 0.0,\n",
       " 'to': 0.0,\n",
       " 'tongue': 0.0,\n",
       " 'toxin': 0.0,\n",
       " 'trouble': 0.0,\n",
       " 'tummy': 0.0,\n",
       " 'unusually': 0.0,\n",
       " 'unwell': 0.0,\n",
       " 'up': 0.0,\n",
       " 'usual': 0.0,\n",
       " 'very': 0.0,\n",
       " 'voice': 0.0,\n",
       " 'vomit': 0.0,\n",
       " 'vomiting': 0.0,\n",
       " 'walk': 0.0,\n",
       " 'wasting': 0.0,\n",
       " 'watering': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'weight': 0.0,\n",
       " 'well': 0.0,\n",
       " 'what': 0.0,\n",
       " 'wheezing': 0.0,\n",
       " 'white': 0.0,\n",
       " 'yellow': 0.0,\n",
       " 'you': 0.0,\n",
       " 'your': 0.0,\n",
       " '™': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg1 = patterns[0][0]\n",
    "print(arg1)\n",
    "dic = {}\n",
    "for i,word in enumerate(all_words):\n",
    "    dic[word] = bag_of_words(arg1, all_words)[i]\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "hidden_sizes = [88,36,len(tags)*2]\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Configure the neural network\n",
    "\n",
    "# define each parameter that is equal to 0 using an empirical value or a value based on your experience\n",
    "# TBD\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 12\n",
    "output_size = len(tags)\n",
    "\n",
    "# STEP 4: Train the model\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer: which one is the best one?\n",
    "# TBD\n",
    "\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_e = 0\n",
    "    for i,(words, labels) in enumerate(train_loader):\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_e += loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i+1 % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: loss {loss_e/(i+1):.3f}')\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Save the model\n",
    "# TBD: name and save the model\n",
    "torch.save(data ,'chat_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Medservice\n",
      "You: hi bot\n",
      "MedAssistant: Hi there, how can I help?\n",
      "You: whats your name\n",
      "MedAssistant: I'm bot!\n",
      "You: im feeling bad\n",
      "MedAssistant: which are your principal symptons?\n",
      "You: a cough\n",
      "MedAssistant: I'm not sure if you have COVID-19 or common cold. Try to say me some extra symptoms please!\n",
      "You: im coughing a lot\n",
      "MedAssistant: I'm not sure if you have COVID-19 or common cold. Try to say me some extra symptoms please!\n",
      "You: i think im coughing more than usual\n",
      "MedAssistant: you might be suffering from COVID-19\n",
      "You: exit\n",
      "Hope it was usefull, I'll be here!\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Test the model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TBD: Load the intents file\n",
    "with open('medical_intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "# TBD: Retrieve the model and all the sizings\n",
    "data = torch.load('chat_data.pth')\n",
    "model_state_dict = data[\"model_state\"]\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "\n",
    "\n",
    "# TBD: build the NN\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "#model = Network().to(device) \n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()\n",
    "\n",
    "# TBD: prepare a command-line conversation (don't forget something to make the user exit the script!)\n",
    "bot_name = \"MedAssistant\"\n",
    "print('Welcome to Medservice')\n",
    "while True:\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence.lower() == \"exit\":\n",
    "        print('Hope it was usefull, I\\'ll be here!')\n",
    "        break\n",
    "    else:\n",
    "        sentence_processed = preprocess(sentence)\n",
    "        sentence_processed = [token.text for token in nlp(sentence_processed)]\n",
    "        X = bag_of_words(sentence_processed, all_words)\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "        \n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=0)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "        probs = torch.softmax(output, dim=0)\n",
    "        prob = probs[predicted.item()]\n",
    "        \n",
    "        if prob.item()>0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if intent['tag'] == tag:\n",
    "                    print(f'{bot_name}: {random.choice(intent[\"responses\"])}')\n",
    "                    \n",
    "        elif sum(torch.softmax(output, dim=0)>0.4).item() > 1:\n",
    "            possible = np.array(tags)[(torch.softmax(output, dim=0)>0.4).cpu().detach().numpy()]\n",
    "            print(f'{bot_name}: I\\'m not sure if you have {possible[0]} or {possible[1]}. Try to say me some extra symptoms please!')\n",
    "        \n",
    "        else:\n",
    "            print(f'{bot_name}: Try to be more specific please!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
