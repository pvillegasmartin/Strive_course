{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary\n",
    "from torch.autograd  import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_akbilgic.csv')\n",
    "\n",
    "df_train = df[:400]  # train split, you can use this method to split your data, #hardcoded for now,you can change it\n",
    "df_test = df[400:] # the most important point is that the latest data is being used for testing\n",
    "\n",
    "\n",
    "# Receives the number of samples (batch_size) of size (n_steps) to extract from the time series and output a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 7)\n",
      "(3, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "def next_stock_batch(batch_size, n_steps, df_base):\n",
    "    \n",
    "    #what would be the relation of x and y: y is of the same size as x, but shifted one period ahead\n",
    "    \n",
    "    t_min = 0\n",
    "    t_max = df_base.shape[0] \n",
    "    \n",
    "    x = np.zeros((batch_size, n_steps, 7))  # 7 columns for the 7 features\n",
    "    y = np.zeros((batch_size, n_steps, 1))\n",
    "    \n",
    "    starting_points = np.random.randint(t_min, t_max - n_steps - 1, size=batch_size) #randomly choose starting points e.g. 14 therfore it is upto 23 if \n",
    "    #the steps are 10 \n",
    "    # online training, you can use this method to generate the starting points, there are no epochs, \n",
    "    # an epoch you go through the entire dataset once\n",
    "    # re-ordeirng the data with epochs -> SGD\n",
    "    # if we go through the same order -> GD\n",
    "    # online training, no epochs, radomly choose starting points, \n",
    "    # the batches are selected randomly from the data set but the sequence is the same\n",
    "    # when should you choose epochs and online training? you should use it when the data is stochastic in nature \n",
    "    # \n",
    "\n",
    "    #how many sequences do we need? depends on the batch_size\n",
    "    \n",
    "    for k in range(batch_size):\n",
    "\n",
    "        lmat = []\n",
    "        for j in np.arange(n_steps+1):   # here is where the y shift is taking place\n",
    "            lmat.append(df_base.iloc[starting_points[k]+j,2:].values)\n",
    "            # appending the intial values to the list, n.b. the value would be +2 on the csv\n",
    "        \n",
    "        lmat = np.array(lmat)  # a sequence\n",
    "        \n",
    "        x[k,:,:] = lmat[:n_steps,1:]  # the first column is the target, the rest are the inputs\n",
    "        y[k,:,0] = lmat[1:n_steps+1,0]  # show on the csv\n",
    "        \n",
    "        \n",
    "    return x, y\n",
    "\n",
    "x,y = next_stock_batch(3, 10, df_train)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)  #batch first returns a different shape structure (b,s,..)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(40 * hidden_dim, 40 * hidden_dim) #batch_size * hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        #intializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # print(batch_size.shape)\n",
    "        # print(x.shape)\n",
    "        # print(hidden.shape)\n",
    "        \n",
    "        #Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        hidden_last = hidden[-1]\n",
    "        out = self.relu(hidden_last.flatten())  #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 40 \n",
    "n_inputs = 7\n",
    "n_neurons = 100\n",
    "hidden_dim = n_steps\n",
    "n_outputs = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_inputs, n_neurons,  n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "optim = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 40, 7])\n",
      "torch.Size([40, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4000) must match the size of tensor b (1600) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-306785f1e54b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearner\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearner\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearner\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2614\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2615\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2616\u001b[1;33m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2617\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2618\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearner\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4000) must match the size of tensor b (1600) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# train_losses = []\n",
    "# #number of batches we will go through\n",
    "# n_iterations = 500\n",
    "\n",
    "# #after how many operations we will print information\n",
    "# printing_gap = 1\n",
    "\n",
    "# for iter in range(n_iterations):\n",
    "#     #get a batch\n",
    "#     x_batch, y_batch = next_stock_batch(batch_size, n_steps, df_train)\n",
    "    \n",
    "#     #make to tensors\n",
    "#     x_batch = torch.from_numpy(x_batch)\n",
    "#     y_batch = torch.from_numpy(y_batch).squeeze(-1) #squeeze removes the last dimension (40,40) instead of (40,40,1)\n",
    "    \n",
    "#     # torch variables\n",
    "#     x_batch = Variable(x_batch).float()\n",
    "#     y_batch = Variable(y_batch).float()\n",
    "    \n",
    "#     #reset the gradients\n",
    "#     optim.zero_grad()\n",
    "    \n",
    "#     #outputs\n",
    "#     print(x_batch.shape)\n",
    "#     print(y_batch.shape)\n",
    "#     output = model(x_batch)\n",
    "    \n",
    "#     #loss\n",
    "#     loss = criterion(output, y_batch.flatten()) \n",
    "    \n",
    "#     #compute gradients\n",
    "#     loss.backward()\n",
    "    \n",
    "#     #apply the gradients\n",
    "#     optim.step() \n",
    "    \n",
    "#     #append loss\n",
    "#     train_losses.append(loss.item())\n",
    "    \n",
    "#     if iter % printing_gap == 0:\n",
    "#         print(\"iter: \", iter, \"\\tloss: \", loss.item())\n",
    "\n",
    "# plt.plot(train_losses, label = \"Train Loss\")\n",
    "# plt.xlabel(\"Iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend(loc = \"best\")\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize our RNN model to pass it to the optimizer\n",
    "hidden_dim = n_steps\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "\n",
    "model = Model(n_inputs, hidden_dim, n_neurons)\n",
    "\n",
    "# What would be an adequate loss function?\n",
    "# criterion = torch.nn.NNNLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "# optimizer to apply the gradients\n",
    "optim = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 \tLoss:  0.0708136260509491\n",
      "iter:  1 \tLoss:  0.5279101729393005\n",
      "iter:  2 \tLoss:  0.41595008969306946\n",
      "iter:  3 \tLoss:  1.0917264223098755\n",
      "iter:  4 \tLoss:  0.2965041399002075\n",
      "iter:  5 \tLoss:  2.60261607170105\n",
      "iter:  6 \tLoss:  0.22001713514328003\n",
      "iter:  7 \tLoss:  2.434274673461914\n",
      "iter:  8 \tLoss:  0.9138883352279663\n",
      "iter:  9 \tLoss:  2.4702632427215576\n",
      "iter:  10 \tLoss:  1.2285218238830566\n",
      "iter:  11 \tLoss:  0.3187585175037384\n",
      "iter:  12 \tLoss:  0.22580312192440033\n",
      "iter:  13 \tLoss:  0.42612889409065247\n",
      "iter:  14 \tLoss:  0.18171419203281403\n",
      "iter:  15 \tLoss:  0.5959367752075195\n",
      "iter:  16 \tLoss:  0.49722588062286377\n",
      "iter:  17 \tLoss:  0.1498907208442688\n",
      "iter:  18 \tLoss:  0.36135560274124146\n",
      "iter:  19 \tLoss:  0.35662075877189636\n",
      "iter:  20 \tLoss:  0.411380410194397\n",
      "iter:  21 \tLoss:  0.17517462372779846\n",
      "iter:  22 \tLoss:  0.319238543510437\n",
      "iter:  23 \tLoss:  0.44252923130989075\n",
      "iter:  24 \tLoss:  0.40737056732177734\n",
      "iter:  25 \tLoss:  0.2005748301744461\n",
      "iter:  26 \tLoss:  0.26850566267967224\n",
      "iter:  27 \tLoss:  0.5736793279647827\n",
      "iter:  28 \tLoss:  0.5379857420921326\n",
      "iter:  29 \tLoss:  0.6347530484199524\n",
      "iter:  30 \tLoss:  0.4311457872390747\n",
      "iter:  31 \tLoss:  0.2744591236114502\n",
      "iter:  32 \tLoss:  0.2322249561548233\n",
      "iter:  33 \tLoss:  0.19108356535434723\n",
      "iter:  34 \tLoss:  0.17139855027198792\n",
      "iter:  35 \tLoss:  0.2488204836845398\n",
      "iter:  36 \tLoss:  0.21389219164848328\n",
      "iter:  37 \tLoss:  0.1228206530213356\n",
      "iter:  38 \tLoss:  0.18801791965961456\n",
      "iter:  39 \tLoss:  0.1478874534368515\n",
      "iter:  40 \tLoss:  0.1506337970495224\n",
      "iter:  41 \tLoss:  0.1606251746416092\n",
      "iter:  42 \tLoss:  0.122280053794384\n",
      "iter:  43 \tLoss:  0.45663315057754517\n",
      "iter:  44 \tLoss:  0.18187890946865082\n",
      "iter:  45 \tLoss:  0.10935132950544357\n",
      "iter:  46 \tLoss:  0.16381524503231049\n",
      "iter:  47 \tLoss:  0.1191173642873764\n",
      "iter:  48 \tLoss:  0.10595086961984634\n",
      "iter:  49 \tLoss:  0.11915367096662521\n",
      "iter:  50 \tLoss:  0.08920212090015411\n",
      "iter:  51 \tLoss:  0.1103631854057312\n",
      "iter:  52 \tLoss:  0.12112447619438171\n",
      "iter:  53 \tLoss:  0.11184833198785782\n",
      "iter:  54 \tLoss:  0.09019231051206589\n",
      "iter:  55 \tLoss:  0.05509166419506073\n",
      "iter:  56 \tLoss:  0.12723897397518158\n",
      "iter:  57 \tLoss:  0.11168674379587173\n",
      "iter:  58 \tLoss:  0.021447528153657913\n",
      "iter:  59 \tLoss:  0.021328650414943695\n",
      "iter:  60 \tLoss:  0.02094094827771187\n",
      "iter:  61 \tLoss:  0.036258138716220856\n",
      "iter:  62 \tLoss:  0.02053391933441162\n",
      "iter:  63 \tLoss:  0.020893264561891556\n",
      "iter:  64 \tLoss:  0.019531680271029472\n",
      "iter:  65 \tLoss:  0.01916670612990856\n",
      "iter:  66 \tLoss:  0.01908312551677227\n",
      "iter:  67 \tLoss:  0.018958089873194695\n",
      "iter:  68 \tLoss:  0.01872835122048855\n",
      "iter:  69 \tLoss:  0.019474143162369728\n",
      "iter:  70 \tLoss:  0.01709282584488392\n",
      "iter:  71 \tLoss:  0.018868006765842438\n",
      "iter:  72 \tLoss:  0.019329797476530075\n",
      "iter:  73 \tLoss:  0.021335585042834282\n",
      "iter:  74 \tLoss:  0.019986765459179878\n",
      "iter:  75 \tLoss:  0.019456272944808006\n",
      "iter:  76 \tLoss:  0.019087351858615875\n",
      "iter:  77 \tLoss:  0.01893419772386551\n",
      "iter:  78 \tLoss:  0.018572771921753883\n",
      "iter:  79 \tLoss:  0.019516414031386375\n",
      "iter:  80 \tLoss:  0.032704927027225494\n",
      "iter:  81 \tLoss:  0.018232449889183044\n",
      "iter:  82 \tLoss:  0.017399663105607033\n",
      "iter:  83 \tLoss:  0.02383686788380146\n",
      "iter:  84 \tLoss:  0.017975561320781708\n",
      "iter:  85 \tLoss:  0.01853528432548046\n",
      "iter:  86 \tLoss:  0.01793820783495903\n",
      "iter:  87 \tLoss:  0.01770176738500595\n",
      "iter:  88 \tLoss:  0.020629482343792915\n",
      "iter:  89 \tLoss:  0.019569776952266693\n",
      "iter:  90 \tLoss:  0.018290746957063675\n",
      "iter:  91 \tLoss:  0.018987495452165604\n",
      "iter:  92 \tLoss:  0.01883748546242714\n",
      "iter:  93 \tLoss:  0.017233623191714287\n",
      "iter:  94 \tLoss:  0.01796562224626541\n",
      "iter:  95 \tLoss:  0.01932429149746895\n",
      "iter:  96 \tLoss:  0.018322329968214035\n",
      "iter:  97 \tLoss:  0.0184133592993021\n",
      "iter:  98 \tLoss:  0.019204728305339813\n",
      "iter:  99 \tLoss:  0.018480248749256134\n",
      "iter:  100 \tLoss:  0.01854204572737217\n",
      "iter:  101 \tLoss:  0.01762145198881626\n",
      "iter:  102 \tLoss:  0.018669018521904945\n",
      "iter:  103 \tLoss:  0.01865725964307785\n",
      "iter:  104 \tLoss:  0.018005097284913063\n",
      "iter:  105 \tLoss:  0.017777297645807266\n",
      "iter:  106 \tLoss:  0.01793145015835762\n",
      "iter:  107 \tLoss:  0.018094686791300774\n",
      "iter:  108 \tLoss:  0.018905002623796463\n",
      "iter:  109 \tLoss:  0.019245099276304245\n",
      "iter:  110 \tLoss:  0.0183420293033123\n",
      "iter:  111 \tLoss:  0.018604330718517303\n",
      "iter:  112 \tLoss:  0.019088909029960632\n",
      "iter:  113 \tLoss:  0.019048212096095085\n",
      "iter:  114 \tLoss:  0.01833859272301197\n",
      "iter:  115 \tLoss:  0.018889455124735832\n",
      "iter:  116 \tLoss:  0.01890811137855053\n",
      "iter:  117 \tLoss:  0.018997281789779663\n",
      "iter:  118 \tLoss:  0.017219452187418938\n",
      "iter:  119 \tLoss:  0.019014138728380203\n",
      "iter:  120 \tLoss:  0.018018607050180435\n",
      "iter:  121 \tLoss:  0.01702629029750824\n",
      "iter:  122 \tLoss:  0.018680855631828308\n",
      "iter:  123 \tLoss:  0.018781272694468498\n",
      "iter:  124 \tLoss:  0.01764661632478237\n",
      "iter:  125 \tLoss:  0.01875258795917034\n",
      "iter:  126 \tLoss:  0.01764005422592163\n",
      "iter:  127 \tLoss:  0.018178587779402733\n",
      "iter:  128 \tLoss:  0.01844462938606739\n",
      "iter:  129 \tLoss:  0.01827639900147915\n",
      "iter:  130 \tLoss:  0.018058275803923607\n",
      "iter:  131 \tLoss:  0.018368680030107498\n",
      "iter:  132 \tLoss:  0.018810249865055084\n",
      "iter:  133 \tLoss:  0.018149996176362038\n",
      "iter:  134 \tLoss:  0.018618466332554817\n",
      "iter:  135 \tLoss:  0.018268287181854248\n",
      "iter:  136 \tLoss:  0.01914120651781559\n",
      "iter:  137 \tLoss:  0.018773827701807022\n",
      "iter:  138 \tLoss:  0.019139070063829422\n",
      "iter:  139 \tLoss:  0.01843801699578762\n",
      "iter:  140 \tLoss:  0.01880013570189476\n",
      "iter:  141 \tLoss:  0.01820613071322441\n",
      "iter:  142 \tLoss:  0.017834628000855446\n",
      "iter:  143 \tLoss:  0.019156360998749733\n",
      "iter:  144 \tLoss:  0.0199241042137146\n",
      "iter:  145 \tLoss:  0.017311956733465195\n",
      "iter:  146 \tLoss:  0.017697911709547043\n",
      "iter:  147 \tLoss:  0.019242191687226295\n",
      "iter:  148 \tLoss:  0.017632799223065376\n",
      "iter:  149 \tLoss:  0.0180585365742445\n",
      "iter:  150 \tLoss:  0.017758294939994812\n",
      "iter:  151 \tLoss:  0.019106756895780563\n",
      "iter:  152 \tLoss:  0.017460020259022713\n",
      "iter:  153 \tLoss:  0.01844901032745838\n",
      "iter:  154 \tLoss:  0.018102504312992096\n",
      "iter:  155 \tLoss:  0.01731458120048046\n",
      "iter:  156 \tLoss:  0.01788337714970112\n",
      "iter:  157 \tLoss:  0.016673846170306206\n",
      "iter:  158 \tLoss:  0.017408253625035286\n",
      "iter:  159 \tLoss:  0.01806245557963848\n",
      "iter:  160 \tLoss:  0.017413297668099403\n",
      "iter:  161 \tLoss:  0.01802820898592472\n",
      "iter:  162 \tLoss:  0.01937827654182911\n",
      "iter:  163 \tLoss:  0.01776074431836605\n",
      "iter:  164 \tLoss:  0.01790946163237095\n",
      "iter:  165 \tLoss:  0.01885930262506008\n",
      "iter:  166 \tLoss:  0.019021157175302505\n",
      "iter:  167 \tLoss:  0.01761040836572647\n",
      "iter:  168 \tLoss:  0.018551962450146675\n",
      "iter:  169 \tLoss:  0.019083332270383835\n",
      "iter:  170 \tLoss:  0.018164537847042084\n",
      "iter:  171 \tLoss:  0.017973242327570915\n",
      "iter:  172 \tLoss:  0.017037570476531982\n",
      "iter:  173 \tLoss:  0.01979810744524002\n",
      "iter:  174 \tLoss:  0.017786338925361633\n",
      "iter:  175 \tLoss:  0.018653109669685364\n",
      "iter:  176 \tLoss:  0.018728576600551605\n",
      "iter:  177 \tLoss:  0.019101297482848167\n",
      "iter:  178 \tLoss:  0.01943165250122547\n",
      "iter:  179 \tLoss:  0.01807352527976036\n",
      "iter:  180 \tLoss:  0.016971614211797714\n",
      "iter:  181 \tLoss:  0.018152568489313126\n",
      "iter:  182 \tLoss:  0.018898863345384598\n",
      "iter:  183 \tLoss:  0.018405670300126076\n",
      "iter:  184 \tLoss:  0.019201982766389847\n",
      "iter:  185 \tLoss:  0.017927261069417\n",
      "iter:  186 \tLoss:  0.01892600953578949\n",
      "iter:  187 \tLoss:  0.017858726903796196\n",
      "iter:  188 \tLoss:  0.017352404072880745\n",
      "iter:  189 \tLoss:  0.018345434218645096\n",
      "iter:  190 \tLoss:  0.018886230885982513\n",
      "iter:  191 \tLoss:  0.01803685538470745\n",
      "iter:  192 \tLoss:  0.01828034594655037\n",
      "iter:  193 \tLoss:  0.018956586718559265\n",
      "iter:  194 \tLoss:  0.018819184973835945\n",
      "iter:  195 \tLoss:  0.01825377717614174\n",
      "iter:  196 \tLoss:  0.0182271059602499\n",
      "iter:  197 \tLoss:  0.018348900601267815\n",
      "iter:  198 \tLoss:  0.01946045458316803\n",
      "iter:  199 \tLoss:  0.019370146095752716\n",
      "iter:  200 \tLoss:  0.01794946938753128\n",
      "iter:  201 \tLoss:  0.018803492188453674\n",
      "iter:  202 \tLoss:  0.01789247617125511\n",
      "iter:  203 \tLoss:  0.018327681347727776\n",
      "iter:  204 \tLoss:  0.018486887216567993\n",
      "iter:  205 \tLoss:  0.017711501568555832\n",
      "iter:  206 \tLoss:  0.017920764163136482\n",
      "iter:  207 \tLoss:  0.01771942712366581\n",
      "iter:  208 \tLoss:  0.01807479001581669\n",
      "iter:  209 \tLoss:  0.018824992701411247\n",
      "iter:  210 \tLoss:  0.018438072875142097\n",
      "iter:  211 \tLoss:  0.018106717616319656\n",
      "iter:  212 \tLoss:  0.0194209273904562\n",
      "iter:  213 \tLoss:  0.018742257729172707\n",
      "iter:  214 \tLoss:  0.01875286176800728\n",
      "iter:  215 \tLoss:  0.019513482227921486\n",
      "iter:  216 \tLoss:  0.017164232209324837\n",
      "iter:  217 \tLoss:  0.018552910536527634\n",
      "iter:  218 \tLoss:  0.018557216972112656\n",
      "iter:  219 \tLoss:  0.01912129484117031\n",
      "iter:  220 \tLoss:  0.019094528630375862\n",
      "iter:  221 \tLoss:  0.017769254744052887\n",
      "iter:  222 \tLoss:  0.018187735229730606\n",
      "iter:  223 \tLoss:  0.017930513247847557\n",
      "iter:  224 \tLoss:  0.018235910683870316\n",
      "iter:  225 \tLoss:  0.018293948844075203\n",
      "iter:  226 \tLoss:  0.01838708482682705\n",
      "iter:  227 \tLoss:  0.018539192155003548\n",
      "iter:  228 \tLoss:  0.01803101971745491\n",
      "iter:  229 \tLoss:  0.018569087609648705\n",
      "iter:  230 \tLoss:  0.018586914986371994\n",
      "iter:  231 \tLoss:  0.01809002459049225\n",
      "iter:  232 \tLoss:  0.018064342439174652\n",
      "iter:  233 \tLoss:  0.017536915838718414\n",
      "iter:  234 \tLoss:  0.018557477742433548\n",
      "iter:  235 \tLoss:  0.018117334693670273\n",
      "iter:  236 \tLoss:  0.018991030752658844\n",
      "iter:  237 \tLoss:  0.018274106085300446\n",
      "iter:  238 \tLoss:  0.018356531858444214\n",
      "iter:  239 \tLoss:  0.017979487776756287\n",
      "iter:  240 \tLoss:  0.01922326162457466\n",
      "iter:  241 \tLoss:  0.019767019897699356\n",
      "iter:  242 \tLoss:  0.01803078129887581\n",
      "iter:  243 \tLoss:  0.01790202409029007\n",
      "iter:  244 \tLoss:  0.018442600965499878\n",
      "iter:  245 \tLoss:  0.018193664029240608\n",
      "iter:  246 \tLoss:  0.018548017367720604\n",
      "iter:  247 \tLoss:  0.018254714086651802\n",
      "iter:  248 \tLoss:  0.019116409122943878\n",
      "iter:  249 \tLoss:  0.018313637003302574\n",
      "iter:  250 \tLoss:  0.018407722935080528\n",
      "iter:  251 \tLoss:  0.017969734966754913\n",
      "iter:  252 \tLoss:  0.018326587975025177\n",
      "iter:  253 \tLoss:  0.01852412335574627\n",
      "iter:  254 \tLoss:  0.019030295312404633\n",
      "iter:  255 \tLoss:  0.018612854182720184\n",
      "iter:  256 \tLoss:  0.018018599599599838\n",
      "iter:  257 \tLoss:  0.018005775287747383\n",
      "iter:  258 \tLoss:  0.018674442544579506\n",
      "iter:  259 \tLoss:  0.01903018169105053\n",
      "iter:  260 \tLoss:  0.01846916228532791\n",
      "iter:  261 \tLoss:  0.017790265381336212\n",
      "iter:  262 \tLoss:  0.018574140965938568\n",
      "iter:  263 \tLoss:  0.01771140843629837\n",
      "iter:  264 \tLoss:  0.017929356545209885\n",
      "iter:  265 \tLoss:  0.019213439896702766\n",
      "iter:  266 \tLoss:  0.018659919500350952\n",
      "iter:  267 \tLoss:  0.018666256219148636\n",
      "iter:  268 \tLoss:  0.017135456204414368\n",
      "iter:  269 \tLoss:  0.01847980171442032\n",
      "iter:  270 \tLoss:  0.01834120601415634\n",
      "iter:  271 \tLoss:  0.019269254058599472\n",
      "iter:  272 \tLoss:  0.018523957580327988\n",
      "iter:  273 \tLoss:  0.018434956669807434\n",
      "iter:  274 \tLoss:  0.018353315070271492\n",
      "iter:  275 \tLoss:  0.018571143969893456\n",
      "iter:  276 \tLoss:  0.01877785474061966\n",
      "iter:  277 \tLoss:  0.018032042309641838\n",
      "iter:  278 \tLoss:  0.018449008464813232\n",
      "iter:  279 \tLoss:  0.019334467127919197\n",
      "iter:  280 \tLoss:  0.01881103217601776\n",
      "iter:  281 \tLoss:  0.018600203096866608\n",
      "iter:  282 \tLoss:  0.018971163779497147\n",
      "iter:  283 \tLoss:  0.018952304497361183\n",
      "iter:  284 \tLoss:  0.01883491314947605\n",
      "iter:  285 \tLoss:  0.01893923059105873\n",
      "iter:  286 \tLoss:  0.01881147362291813\n",
      "iter:  287 \tLoss:  0.019982868805527687\n",
      "iter:  288 \tLoss:  0.019014526158571243\n",
      "iter:  289 \tLoss:  0.018281958997249603\n",
      "iter:  290 \tLoss:  0.018714221194386482\n",
      "iter:  291 \tLoss:  0.019296027719974518\n",
      "iter:  292 \tLoss:  0.017792465165257454\n",
      "iter:  293 \tLoss:  0.018880439922213554\n",
      "iter:  294 \tLoss:  0.018058136105537415\n",
      "iter:  295 \tLoss:  0.0170234777033329\n",
      "iter:  296 \tLoss:  0.017893297597765923\n",
      "iter:  297 \tLoss:  0.01820356771349907\n",
      "iter:  298 \tLoss:  0.018364977091550827\n",
      "iter:  299 \tLoss:  0.01835475116968155\n",
      "iter:  300 \tLoss:  0.01832657866179943\n",
      "iter:  301 \tLoss:  0.018517039716243744\n",
      "iter:  302 \tLoss:  0.019436050206422806\n",
      "iter:  303 \tLoss:  0.01850946433842182\n",
      "iter:  304 \tLoss:  0.018009310588240623\n",
      "iter:  305 \tLoss:  0.019838495180010796\n",
      "iter:  306 \tLoss:  0.019865768030285835\n",
      "iter:  307 \tLoss:  0.019119566306471825\n",
      "iter:  308 \tLoss:  0.018999679014086723\n",
      "iter:  309 \tLoss:  0.019388295710086823\n",
      "iter:  310 \tLoss:  0.0181933231651783\n",
      "iter:  311 \tLoss:  0.018624482676386833\n",
      "iter:  312 \tLoss:  0.017779944464564323\n",
      "iter:  313 \tLoss:  0.018351266160607338\n",
      "iter:  314 \tLoss:  0.017993204295635223\n",
      "iter:  315 \tLoss:  0.018662191927433014\n",
      "iter:  316 \tLoss:  0.019038505852222443\n",
      "iter:  317 \tLoss:  0.0182478204369545\n",
      "iter:  318 \tLoss:  0.019895633682608604\n",
      "iter:  319 \tLoss:  0.018444849178195\n",
      "iter:  320 \tLoss:  0.017530839890241623\n",
      "iter:  321 \tLoss:  0.017235277220606804\n",
      "iter:  322 \tLoss:  0.017748966813087463\n",
      "iter:  323 \tLoss:  0.01837238110601902\n",
      "iter:  324 \tLoss:  0.017422225326299667\n",
      "iter:  325 \tLoss:  0.01807865872979164\n",
      "iter:  326 \tLoss:  0.018324581906199455\n",
      "iter:  327 \tLoss:  0.019012469798326492\n",
      "iter:  328 \tLoss:  0.018287522718310356\n",
      "iter:  329 \tLoss:  0.01787826605141163\n",
      "iter:  330 \tLoss:  0.01838855817914009\n",
      "iter:  331 \tLoss:  0.019384920597076416\n",
      "iter:  332 \tLoss:  0.018093114718794823\n",
      "iter:  333 \tLoss:  0.0179327130317688\n",
      "iter:  334 \tLoss:  0.018019692972302437\n",
      "iter:  335 \tLoss:  0.018655652180314064\n",
      "iter:  336 \tLoss:  0.018403250724077225\n",
      "iter:  337 \tLoss:  0.018511513248085976\n",
      "iter:  338 \tLoss:  0.018378971144557\n",
      "iter:  339 \tLoss:  0.018398070707917213\n",
      "iter:  340 \tLoss:  0.018188543617725372\n",
      "iter:  341 \tLoss:  0.018263474106788635\n",
      "iter:  342 \tLoss:  0.01797056384384632\n",
      "iter:  343 \tLoss:  0.019143464043736458\n",
      "iter:  344 \tLoss:  0.018864847719669342\n",
      "iter:  345 \tLoss:  0.01970420777797699\n",
      "iter:  346 \tLoss:  0.017530744895339012\n",
      "iter:  347 \tLoss:  0.017604203894734383\n",
      "iter:  348 \tLoss:  0.018471132963895798\n",
      "iter:  349 \tLoss:  0.017901431769132614\n",
      "iter:  350 \tLoss:  0.019392652437090874\n",
      "iter:  351 \tLoss:  0.01851275935769081\n",
      "iter:  352 \tLoss:  0.018778279423713684\n",
      "iter:  353 \tLoss:  0.019005121663212776\n",
      "iter:  354 \tLoss:  0.01869659125804901\n",
      "iter:  355 \tLoss:  0.01783447526395321\n",
      "iter:  356 \tLoss:  0.017436092719435692\n",
      "iter:  357 \tLoss:  0.017822347581386566\n",
      "iter:  358 \tLoss:  0.01954052783548832\n",
      "iter:  359 \tLoss:  0.019045168533921242\n",
      "iter:  360 \tLoss:  0.018517255783081055\n",
      "iter:  361 \tLoss:  0.019104287028312683\n",
      "iter:  362 \tLoss:  0.01933165267109871\n",
      "iter:  363 \tLoss:  0.018757138401269913\n",
      "iter:  364 \tLoss:  0.01809007115662098\n",
      "iter:  365 \tLoss:  0.018953001126646996\n",
      "iter:  366 \tLoss:  0.01997019723057747\n",
      "iter:  367 \tLoss:  0.019247151911258698\n",
      "iter:  368 \tLoss:  0.018129544332623482\n",
      "iter:  369 \tLoss:  0.018179945647716522\n",
      "iter:  370 \tLoss:  0.019900387153029442\n",
      "iter:  371 \tLoss:  0.019404403865337372\n",
      "iter:  372 \tLoss:  0.019231067970395088\n",
      "iter:  373 \tLoss:  0.01788746565580368\n",
      "iter:  374 \tLoss:  0.018505776301026344\n",
      "iter:  375 \tLoss:  0.019543441012501717\n",
      "iter:  376 \tLoss:  0.018427768722176552\n",
      "iter:  377 \tLoss:  0.01758948713541031\n",
      "iter:  378 \tLoss:  0.018398739397525787\n",
      "iter:  379 \tLoss:  0.018063629046082497\n",
      "iter:  380 \tLoss:  0.017857082188129425\n",
      "iter:  381 \tLoss:  0.01934242993593216\n",
      "iter:  382 \tLoss:  0.0190139077603817\n",
      "iter:  383 \tLoss:  0.01986304484307766\n",
      "iter:  384 \tLoss:  0.01971215195953846\n",
      "iter:  385 \tLoss:  0.01773311197757721\n",
      "iter:  386 \tLoss:  0.019056780263781548\n",
      "iter:  387 \tLoss:  0.019690822809934616\n",
      "iter:  388 \tLoss:  0.017931008711457253\n",
      "iter:  389 \tLoss:  0.01791846752166748\n",
      "iter:  390 \tLoss:  0.017903784289956093\n",
      "iter:  391 \tLoss:  0.01691618748009205\n",
      "iter:  392 \tLoss:  0.01865028403699398\n",
      "iter:  393 \tLoss:  0.018467126414179802\n",
      "iter:  394 \tLoss:  0.0181655865162611\n",
      "iter:  395 \tLoss:  0.018103240057826042\n",
      "iter:  396 \tLoss:  0.019176965579390526\n",
      "iter:  397 \tLoss:  0.01818476989865303\n",
      "iter:  398 \tLoss:  0.016817571595311165\n",
      "iter:  399 \tLoss:  0.01908962056040764\n",
      "iter:  400 \tLoss:  0.017558550462126732\n",
      "iter:  401 \tLoss:  0.017649950459599495\n",
      "iter:  402 \tLoss:  0.018570834770798683\n",
      "iter:  403 \tLoss:  0.018322017043828964\n",
      "iter:  404 \tLoss:  0.019032446667551994\n",
      "iter:  405 \tLoss:  0.017580492421984673\n",
      "iter:  406 \tLoss:  0.019006503745913506\n",
      "iter:  407 \tLoss:  0.01794484071433544\n",
      "iter:  408 \tLoss:  0.01929956115782261\n",
      "iter:  409 \tLoss:  0.01778513938188553\n",
      "iter:  410 \tLoss:  0.01897369511425495\n",
      "iter:  411 \tLoss:  0.018294911831617355\n",
      "iter:  412 \tLoss:  0.0181746743619442\n",
      "iter:  413 \tLoss:  0.018644699826836586\n",
      "iter:  414 \tLoss:  0.018752411007881165\n",
      "iter:  415 \tLoss:  0.018566681072115898\n",
      "iter:  416 \tLoss:  0.017961950972676277\n",
      "iter:  417 \tLoss:  0.019439583644270897\n",
      "iter:  418 \tLoss:  0.017555611208081245\n",
      "iter:  419 \tLoss:  0.018803998827934265\n",
      "iter:  420 \tLoss:  0.018969781696796417\n",
      "iter:  421 \tLoss:  0.020471161231398582\n",
      "iter:  422 \tLoss:  0.018112342804670334\n",
      "iter:  423 \tLoss:  0.01866670325398445\n",
      "iter:  424 \tLoss:  0.01870550960302353\n",
      "iter:  425 \tLoss:  0.01793774403631687\n",
      "iter:  426 \tLoss:  0.018855754286050797\n",
      "iter:  427 \tLoss:  0.019872747361660004\n",
      "iter:  428 \tLoss:  0.019421683624386787\n",
      "iter:  429 \tLoss:  0.01914679817855358\n",
      "iter:  430 \tLoss:  0.01937432773411274\n",
      "iter:  431 \tLoss:  0.019187698140740395\n",
      "iter:  432 \tLoss:  0.01814837008714676\n",
      "iter:  433 \tLoss:  0.01826947182416916\n",
      "iter:  434 \tLoss:  0.01901782862842083\n",
      "iter:  435 \tLoss:  0.019466059282422066\n",
      "iter:  436 \tLoss:  0.019352935254573822\n",
      "iter:  437 \tLoss:  0.017499880865216255\n",
      "iter:  438 \tLoss:  0.01873980090022087\n",
      "iter:  439 \tLoss:  0.017511604353785515\n",
      "iter:  440 \tLoss:  0.01957661658525467\n",
      "iter:  441 \tLoss:  0.01791675016283989\n",
      "iter:  442 \tLoss:  0.018059568479657173\n",
      "iter:  443 \tLoss:  0.017615940421819687\n",
      "iter:  444 \tLoss:  0.018656043335795403\n",
      "iter:  445 \tLoss:  0.019075149670243263\n",
      "iter:  446 \tLoss:  0.018149979412555695\n",
      "iter:  447 \tLoss:  0.018402062356472015\n",
      "iter:  448 \tLoss:  0.017702333629131317\n",
      "iter:  449 \tLoss:  0.01917926035821438\n",
      "iter:  450 \tLoss:  0.01942325197160244\n",
      "iter:  451 \tLoss:  0.018241891637444496\n",
      "iter:  452 \tLoss:  0.017805088311433792\n",
      "iter:  453 \tLoss:  0.01925262250006199\n",
      "iter:  454 \tLoss:  0.019093617796897888\n",
      "iter:  455 \tLoss:  0.01880059950053692\n",
      "iter:  456 \tLoss:  0.01947043463587761\n",
      "iter:  457 \tLoss:  0.018646737560629845\n",
      "iter:  458 \tLoss:  0.018535375595092773\n",
      "iter:  459 \tLoss:  0.019419215619564056\n",
      "iter:  460 \tLoss:  0.018601596355438232\n",
      "iter:  461 \tLoss:  0.01849612221121788\n",
      "iter:  462 \tLoss:  0.018174011260271072\n",
      "iter:  463 \tLoss:  0.01827937550842762\n",
      "iter:  464 \tLoss:  0.018364056944847107\n",
      "iter:  465 \tLoss:  0.017775775864720345\n",
      "iter:  466 \tLoss:  0.019452931359410286\n",
      "iter:  467 \tLoss:  0.020720550790429115\n",
      "iter:  468 \tLoss:  0.017863092944025993\n",
      "iter:  469 \tLoss:  0.017761467024683952\n",
      "iter:  470 \tLoss:  0.01870637573301792\n",
      "iter:  471 \tLoss:  0.018615547567605972\n",
      "iter:  472 \tLoss:  0.018206758424639702\n",
      "iter:  473 \tLoss:  0.018821043893694878\n",
      "iter:  474 \tLoss:  0.018752053380012512\n",
      "iter:  475 \tLoss:  0.017795752733945847\n",
      "iter:  476 \tLoss:  0.018051596358418465\n",
      "iter:  477 \tLoss:  0.018196485936641693\n",
      "iter:  478 \tLoss:  0.0187971368432045\n",
      "iter:  479 \tLoss:  0.017297975718975067\n",
      "iter:  480 \tLoss:  0.018928520381450653\n",
      "iter:  481 \tLoss:  0.019208483397960663\n",
      "iter:  482 \tLoss:  0.018614977598190308\n",
      "iter:  483 \tLoss:  0.017954159528017044\n",
      "iter:  484 \tLoss:  0.019165590405464172\n",
      "iter:  485 \tLoss:  0.01850958913564682\n",
      "iter:  486 \tLoss:  0.01946241222321987\n",
      "iter:  487 \tLoss:  0.017841191962361336\n",
      "iter:  488 \tLoss:  0.017807012423872948\n",
      "iter:  489 \tLoss:  0.018639834597706795\n",
      "iter:  490 \tLoss:  0.018211618065834045\n",
      "iter:  491 \tLoss:  0.019089799374341965\n",
      "iter:  492 \tLoss:  0.018749231472611427\n",
      "iter:  493 \tLoss:  0.018404217436909676\n",
      "iter:  494 \tLoss:  0.018711749464273453\n",
      "iter:  495 \tLoss:  0.01816755346953869\n",
      "iter:  496 \tLoss:  0.018279440701007843\n",
      "iter:  497 \tLoss:  0.01937088370323181\n",
      "iter:  498 \tLoss:  0.01927291974425316\n",
      "iter:  499 \tLoss:  0.017997590824961662\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#number of batches we will go through\n",
    "n_iterations = 500\n",
    "#how many squence there will be in a bacth\n",
    "# batch_size = 50\n",
    "#after how many operations we will print information\n",
    "printing_gap = 1\n",
    "\n",
    "#We will store the loss values here to plot them\n",
    "# train_loss = []\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "for iter in range(n_iterations):\n",
    "    #Get a batch\n",
    "    x_batch, y_batch = next_stock_batch(batch_size, n_steps, df_train)\n",
    "    #make into tensor\n",
    "    x_batch = torch.from_numpy(x_batch)\n",
    "    y_batch = torch.from_numpy(y_batch).squeeze(-1)\n",
    "    #make them into torch variables in float format\n",
    "    x_batch = Variable(x_batch).float()\n",
    "    y_batch = Variable(y_batch).float()\n",
    "    #Reset the gradients\n",
    "    optim.zero_grad()\n",
    "    #Get the outputs\n",
    "    # print(x_batch.shape)\n",
    "    # print(y_batch.shape)\n",
    "    output = model(x_batch) \n",
    "    #compute the loss\n",
    "    loss = criterion(output, y_batch.flatten())\n",
    "    #compute the gradients\n",
    "    loss.backward()\n",
    "    #Apply the gradients\n",
    "    optim.step()\n",
    "\n",
    "    #Append the loss value\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    if iter % printing_gap == 0:\n",
    "        #Print the information\n",
    "        print('iter: ', iter, \"\\tLoss: \", loss.item())\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(train_loss, label= \"Train Loss\")\n",
    "plt.xlabel(\" Iteration \")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
